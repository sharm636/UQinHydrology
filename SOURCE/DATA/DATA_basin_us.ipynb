{"cells":[{"cell_type":"markdown","metadata":{"id":"Z_OeB--gT6w7"},"source":["# LOAD LIBRARIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaNysIEcT6w9"},"outputs":[],"source":["import sys\n","sys.path.append(\"../\")\n","print(sys.path)\n","import os\n","import copy\n","import numpy as np\n","import pandas as pd\n","import random\n","import config\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","if not os.path.exists(config.NUMPY_DIR):\n","    os.makedirs(config.NUMPY_DIR)\n","if not os.path.exists(os.path.join(config.RESULT_DIR)):\n","    os.makedirs(os.path.join(config.RESULT_DIR))\n","\n","print(config.NUMPY_DIR)"]},{"cell_type":"markdown","metadata":{"id":"UmLwWB13T6w-"},"source":["# MERGE DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhJWTKZKT6w-"},"outputs":[],"source":["wv_dir = os.path.join(config.DATA_DIR, \"basin_timeseries_v1p2_metForcing_obsFlow/basin_dataset_public_v1p2/basin_mean_forcing/nldas\")\n","sf_dir = os.path.join(config.DATA_DIR, \"basin_timeseries_v1p2_metForcing_obsFlow/basin_dataset_public_v1p2/usgs_streamflow\")\n","\n","dir_list = sorted(set(os.listdir(wv_dir)).intersection(os.listdir(sf_dir)))\n","\n","for directory in dir_list:\n","    print(directory)\n","    wv_file_list = [file for file in sorted(os.listdir(os.path.join(wv_dir, directory))) if file.endswith(\".txt\")]\n","    sf_file_list = [file for file in sorted(os.listdir(os.path.join(sf_dir, directory))) if file.endswith(\".txt\")]    \n","    basin_list = sorted(set([file.split(\"_\")[0] for file in wv_file_list]).intersection([file.split(\"_\")[0] for file in sf_file_list]))\n","    \n","    for basin in basin_list:\n","        wv_file = os.path.join(wv_dir, directory, \"{}_lump_nldas_forcing_leap.txt\".format(basin))\n","        sf_file = os.path.join(sf_dir, directory, \"{}_streamflow_qc.txt\".format(basin))\n","        \n","        with open(wv_file, 'r') as fp:\n","            content = fp.readlines()\n","            area = int(content[2])\n","\n","        wv_df = pd.read_csv(wv_file, sep='\\s+', header=3)\n","        wv_dates = (wv_df.Year.map(str) + \"/\" + wv_df.Mnth.map(str) + \"/\" + wv_df.Day.map(str))\n","        wv_df.index = pd.to_datetime(wv_dates, format=\"%Y/%m/%d\")\n","        \n","        sf_df = pd.read_csv(sf_file, sep='\\s+', header=None)\n","        sf_dates = (sf_df[1].map(str) + \"/\" + sf_df[2].map(str) + \"/\" + sf_df[3].map(str))\n","        sf_df.index = pd.to_datetime(sf_dates, format=\"%Y/%m/%d\")\n","\n","        df = wv_df.join(sf_df[4])\n","        df = df.rename(columns = {4:'SF'})\n","        df['SF'] = df['SF'].fillna(-999.0)\n","        sf_vals = df.SF\n","        nan_idx = sf_vals==-999\n","        sf_vals = 28316846.592 * sf_vals * 86400 / (area * 10**6)\n","        sf_vals[nan_idx] = -999\n","        df.SF = sf_vals\n","\n","        df = df[['PRCP(mm/day)', 'SRAD(W/m2)', 'Tmax(C)',  'Tmin(C)',  'Vp(Pa)', 'SF']]\n","        path = os.path.join(config.DATA_DIR, \"RAW_DATA\", \"{}.txt\".format(basin))\n","        df.to_csv(path, sep='\\t', index_label=\"Date\")"]},{"cell_type":"markdown","metadata":{"id":"_jFEsJnIT6w_"},"source":["# SAVE DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNUoV7OzT6w_"},"outputs":[],"source":["# Static Features\n","basin_attr = pd.read_csv(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"physic_27.csv\"), dtype = {'gauge_id': object})\n","\n","basin_df = pd.read_csv(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"{}.txt\".format(basin_attr.gauge_id.values[0])), sep='\\t', index_col=0)\n","\n","columns = list(basin_attr.columns.values[1:]) + list(basin_df.columns.values)\n","date = np.array([datetime.strptime(day, '%Y-%m-%d').date() for day in basin_df.index.values])\n","\n","data = np.zeros((len(basin_attr), basin_df.shape[0], len(columns)))\n","for i,basin in enumerate(basin_attr.gauge_id.values):\n","    basin_df = pd.read_csv(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"{}.txt\".format(basin)), sep='\\t', index_col=0)\n","    weather_drivers = basin_df.values\n","    static_features = basin_attr.iloc[i].values\n","    static_features = np.repeat(static_features[np.newaxis, :], weather_drivers.shape[0], axis=0)\n","    basin_data = np.concatenate((static_features[:,1:], weather_drivers), axis=1)\n","    data[i] = basin_data\n","print(data.shape)\n","\n","np.save(os.path.join(config.RESULT_DIR, \"Basin_List\"), basin_attr.gauge_id.values)\n","# np.save(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"feature_names.npy\"), columns)\n","# np.save(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"dates.npy\"), date)\n","# np.save(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"data.npy\"), data.astype(np.float32))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptdpdIHeT6xA"},"outputs":[],"source":["Basin_list =np.load(os.path.join(config.RESULT_DIR, \"Basin_List.npy\"), allow_pickle=True)\n","Basin_list"]},{"cell_type":"markdown","metadata":{"id":"vTE2tfkYT6xA"},"source":["# LOAD DATA & REMOVE LEAP YEAR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_j4ZpddT6xB"},"outputs":[],"source":["data = np.load(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"data.npy\")).astype(np.float32)\n","date = np.load(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"dates.npy\"), allow_pickle=True)\n","print(date.shape, data.shape, \"Original data\")\n","\n","date_df = pd.DataFrame(date)\n","date_df = pd.to_datetime(date_df[0], errors='coerce')\n","print(date_df.loc[date_df.dt.month == 2].loc[date_df.dt.day == 29])\n","date = np.delete(date, date_df.loc[date_df.dt.month == 2].loc[date_df.dt.day == 29].index.values)\n","data = np.delete(data, date_df.loc[date_df.dt.month == 2].loc[date_df.dt.day == 29].index.values, axis=1)\n","print(date.shape, data.shape, \"Leap year removed data\")\n","\n","feature_names = np.load(os.path.join(config.DATA_DIR, \"RAW_DATA\", \"feature_names.npy\"), allow_pickle=True)"]},{"cell_type":"markdown","metadata":{"id":"A_N7vUGqT6xB"},"source":["# NAN VALUES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtwE3akHT6xB"},"outputs":[],"source":["data[data==config.unknown] = np.nan\n","for feature,channel in enumerate(config.channels):\n","    print(\"#NaNs:{}\\tChannel:{}\".format(np.sum(np.isnan(data[:,:,feature])), feature_names[channel]))\n","data[np.isnan(data)] = config.unknown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jo_9gGtLT6xB"},"outputs":[],"source":["print(data.shape)\n","res = np.argwhere(data == 0)\n","res"]},{"cell_type":"markdown","metadata":{"id":"B-XCB56ZT6xC"},"source":["# NORMALIZE DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"347i1SIBT6xC"},"outputs":[],"source":["print(\"Original Feature stats\")\n","data[data==config.unknown] = np.nan\n","\n","\n","\n","for feature,channel in enumerate(config.channels):\n","    print(\"Mean:{:.4f}\\tStddev:{:.4f}\\tMin:{:.4f}\\tMax:{:.4f}\\tChannel:{}\".format(np.nanmean(data[:,:,feature]), np.nanstd(data[:,:,feature]), np.nanmin(data[:,:,feature]), np.nanmax(data[:,:,feature]), feature_names[channel]))\n","data[np.isnan(data)] = config.unknown\n","# mean_real = np.nanmean(data, axis=(0,1))\n","# print(mean_real)\n","# std_real = np.nanstd(data, axis=(0,1))\n","# np.save(os.path.join(config.RESULT_DIR, \"Mean__original_data\"), mean_real)\n","# np.save(os.path.join(config.RESULT_DIR, \"std_original_data\"), std_real)\n","\n","# LOG TRANSFORM OF STREAMFLOWS\n","normalized_data = data.copy()\n","normalized_data[:,:,-1][normalized_data[:,:,-1]!=config.unknown] = np.log10(config.add+normalized_data[:,:,-1][normalized_data[:,:,-1]!=config.unknown])\n","\n","print(\"Log Transform Feature stats\")\n","normalized_data[normalized_data==config.unknown] = np.nan\n","for feature,channel in enumerate(config.channels):\n","    print(\"Mean:{:.4f}\\tStddev:{:.4f}\\tMin:{:.4f}\\tMax:{:.4f}\\tChannel:{}\".format(np.nanmean(normalized_data[:,:,feature]), np.nanstd(normalized_data[:,:,feature]), np.nanmin(normalized_data[:,:,feature]), np.nanmax(normalized_data[:,:,feature]), feature_names[channel]))\n","normalized_data[np.isnan(normalized_data)] = config.unknown\n","\n","# Z NORMALIZE DATA\n","normalized_data[normalized_data==config.unknown] = np.nan\n","mean = np.reshape(np.nanmean(normalized_data, axis=(0,1)), (1,1,-1))\n","std = np.reshape(np.nanstd(normalized_data, axis=(0,1)), (1,1,-1))\n","np.save(os.path.join(config.RESULT_DIR, \"Mean__original_data\"), mean)\n","np.save(os.path.join(config.RESULT_DIR, \"std_original_data\"), std)\n","\n","normalized_data = (normalized_data-mean)/std\n","normalized_data[np.isnan(normalized_data)] = config.unknown\n","\n","print(\"Normalized Feature stats\")\n","normalized_data[normalized_data==config.unknown] = np.nan\n","for feature,channel in enumerate(config.channels):\n","    print(\"Mean:{:.4f}\\tStddev:{:.4f}\\tMin:{:.4f}\\tMax:{:.4f}\\tChannel:{}\".format(np.nanmean(normalized_data[:,:,feature]), np.nanstd(normalized_data[:,:,feature]), np.nanmin(normalized_data[:,:,feature]), np.nanmax(normalized_data[:,:,feature]), feature_names[channel]))\n","normalized_data[np.isnan(normalized_data)] = config.unknown\n","normalized_data[np.isnan(normalized_data)].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8V7A7JM5T6xC"},"outputs":[],"source":["normalized_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"immq2YA-T6xC"},"outputs":[],"source":["# import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ng_t4s9T6xD"},"outputs":[],"source":["# plt.figure(figsize=(9, 9))   \n","# plt.hist(data[:,0,9], color = 'blue', edgecolor = 'black',bins = 50)\n","# plt.savefig(os.path.join(config.RESULT_DIR,\"carbonate_rocks_distribution.png\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1JI6JLAT6xD"},"outputs":[],"source":["# plt.figure(figsize=(9, 9))   \n","# #plt.hist(data[:,0,9], color = 'blue', edgecolor = 'black',bins = 100)\n","# sns.kdeplot(data[:,0,9])\n","# plt.savefig(os.path.join(config.RESULT_DIR,\"carbonate_rocks_distribution.png\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EukD-v95T6xD"},"outputs":[],"source":["# Generate extra layer\n","layer = np.repeat(np.arange(1, 532)[:,None], normalized_data.shape[1], axis=1)\n","print(layer.shape)\n","# Get dimensions right...\n","layer = np.expand_dims(layer, axis=2)\n","\n","# ... and finally append to data\n","normalized_data = np.append(normalized_data, layer, axis=2)\n","normalized_data.shape\n","normalized_data[:,0,33]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZPYoKBpT6xD"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMCCVPI8T6xD"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"rVfyFMcIT6xD"},"source":["# CREATE TRAIN DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUmd5rveT6xE"},"outputs":[],"source":["# number_of_rows = normalized_data.shape[0]\n","\n","# random_indices = np.random.choice(number_of_rows, size=500, replace=False)\n","\n","# normalized_data = normalized_data[random_indices,:]\n","# normalized_data.shape\n","\n","np.random.seed(4)\n","np.random.shuffle(normalized_data)\n","normalized_data.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qb5DQ2PJT6xE"},"outputs":[],"source":["normalized_data[:,0,33]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1GA7glVT6xE"},"outputs":[],"source":["basin_order = normalized_data[:,0,33].astype(int).tolist()\n","basin_order = [x - 1 for x in basin_order]\n","Basin_list[basin_order]\n","import pickle\n","with open(os.path.join(config.NUMPY_DIR, \"basin_list_ealstm\"), \"wb\") as fp:   #pickling\n","    basin_list = pickle.dump(Basin_list[basin_order],fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcCA2EJKT6xE"},"outputs":[],"source":["with open(os.path.join(config.NUMPY_DIR, \"basin_list_ealstm\"), \"rb\") as fp:   #pickling\n","    basin_list = pickle.load(fp)\n","basin_list    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3pAlZeBT6xE"},"outputs":[],"source":["# 'train_start': pd.to_datetime('01101999', format='%d%m%Y'),\n","# 'train_end': pd.to_datetime('30092008', format='%d%m%Y'),\n","# 'val_start': pd.to_datetime('01101989', format='%d%m%Y'),\n","# 'val_end': pd.to_datetime('30091999', format='%d%m%Y')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCSL29TIT6xF"},"outputs":[],"source":["\n","train_date = date[np.where(date==pd.Timestamp(2001, 10,1, 0))[0][0]:np.where(date==pd.Timestamp(2008, 9, 30, 0))[0][0]+1]\n","train_data = normalized_data[:,np.where(date==pd.Timestamp(2001, 10,1, 0))[0][0]:np.where(date==pd.Timestamp(2008, 9, 30, 0))[0][0]+1]\n","strided_train_data = np.zeros((train_data.shape[0], 2*(len(train_date)//config.window), config.window, train_data.shape[-1]))\n","print(train_date.shape, train_data.shape, strided_train_data.shape)\n","\n","i = 0\n","k = 0\n","while i<len(train_date):\n","    strided_train_data[:, k] = train_data[:, i:i+config.window]\n","    k += 1\n","\n","    stride = config.window//2\n","    if strided_train_data[:, k].shape == train_data[:, i+stride:i+stride+config.window].shape:\n","        strided_train_data[:, k] = train_data[:, i+stride:i+stride+config.window]\n","        k += 1\n","\n","    i = i+config.window\n","#strided_train_data = strided_train_data[:300, :k]\n","strided_train_data = strided_train_data[:, :k]\n","print(\"Train:{}\".format(strided_train_data.shape))"]},{"cell_type":"markdown","metadata":{"id":"LRW4RVZeT6xF"},"source":["# CREATE VALIDATION DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpOOpP6tT6xF"},"outputs":[],"source":["validation_date = date[np.where(date==pd.Timestamp(1999, 10,1, 0))[0][0]:np.where(date==pd.Timestamp(2001, 9, 30, 0))[0][0]+1]\n","validation_data = normalized_data[:,np.where(date==pd.Timestamp(1999, 10,1, 0))[0][0]:np.where(date==pd.Timestamp(2001, 9, 30, 0))[0][0]+1]\n","strided_validation_data = np.zeros((validation_data.shape[0], 2*(len(validation_date)//config.window), config.window, validation_data.shape[-1]))\n","print(validation_date.shape, validation_data.shape, strided_validation_data.shape)\n","\n","i = 0\n","k = 0\n","while i<len(validation_date):\n","    strided_validation_data[:, k] = validation_data[:, i:i+config.window]\n","    k += 1\n","\n","    stride = config.window//2\n","    if strided_validation_data[:, k].shape == validation_data[:, i+stride:i+stride+config.window].shape:\n","        strided_validation_data[:, k] = validation_data[:, i+stride:i+stride+config.window]\n","        k += 1\n","\n","    i = i+config.window\n","#strided_validation_data = strided_validation_data[:300, :k]\n","strided_validation_data = strided_validation_data[:, :k]\n","print(\"validation:{}\".format(strided_validation_data.shape))"]},{"cell_type":"markdown","metadata":{"id":"kke0VLwBT6xF"},"source":["# CREATE TEST DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mucd4i0xT6xF"},"outputs":[],"source":["test_date = date[np.where(date==pd.Timestamp(1989, 10,1, 0))[0][0]:np.where(date==pd.Timestamp(1999, 9, 30, 0))[0][0]+1]\n","\n","test_data = normalized_data[:,np.where(date==pd.Timestamp(1989, 10,1, 0))[0][0]:np.where(date==pd.Timestamp(1999, 9, 30, 0))[0][0]+1]\n","print(test_data.shape)\n","strided_test_data = np.zeros((test_data.shape[0], 2*(len(test_date)//config.window), config.window, test_data.shape[-1]))\n","print(test_date.shape, test_data.shape, strided_test_data.shape)\n","\n","i = 0\n","k = 0\n","while i<len(test_date):\n","    strided_test_data[:, k] = test_data[:, i:i+config.window]\n","    k += 1\n","\n","    stride = config.window//2\n","    if strided_test_data[:, k].shape == test_data[:, i+stride:i+stride+config.window].shape:\n","        strided_test_data[:, k] = test_data[:, i+stride:i+stride+config.window]\n","        k += 1\n","\n","    i = i+config.window\n","#strided_test_data = strided_test_data[300:, :k]\n","strided_test_data = strided_test_data[:, :k]\n","print(\"test:{}\".format(strided_test_data.shape))"]},{"cell_type":"markdown","metadata":{"id":"mXu1AcexT6xF"},"source":["# SAVE DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbz0h0TbT6xG"},"outputs":[],"source":["print(\"Data:{}\\tTrain_Basin\".format(strided_train_data.shape))\n","np.save(os.path.join(config.NUMPY_DIR, \"train_data_basin_ealstm\"), strided_train_data.astype(np.float32))\n","print(\"Data:{}\\tValidation_Basin\".format(strided_validation_data.shape))\n","np.save(os.path.join(config.NUMPY_DIR, \"validation_data_basin_ealstm\"), strided_validation_data.astype(np.float32))\n","print(\"Data:{}\\tTest_Basin\".format(strided_test_data.shape))\n","np.save(os.path.join(config.NUMPY_DIR, \"test_data_basin_ealstm\"), strided_test_data.astype(np.float32))\n","# print(\"Data:{}\\tTrain_hidden_Basin\".format(strided_hidden_train_data.shape))\n","# np.save(os.path.join(config.NUMPY_DIR, \"train_hidden_data_basin\"), strided_hidden_train_data.astype(np.float32))"]},{"cell_type":"markdown","metadata":{"id":"RIaFzj3KT6xG"},"source":["# AVERAGE STREAMFLOWS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xxxETwjT6xG"},"outputs":[],"source":["node_flow = np.zeros((data.shape[0], 3))\n","for node in range(data.shape[0]):\n","    node_labels = data[node,:,-1]\n","    mean = np.mean(node_labels[node_labels!=config.unknown])\n","    median = np.median(node_labels[node_labels!=config.unknown])\n","    node_flow[node,0] = node\n","    node_flow[node,1] = mean\n","    node_flow[node,2] = median\n","\n","indices = np.argsort(node_flow[:,1])\n","node_flow = node_flow[indices]\n","\n","print(\"Min Obs:{:.4f}\\tNode:{}\".format(node_flow[0,1], node_flow[0,0]))\n","print(\"Max Obs:{:.4f}\\tNode:{}\".format(node_flow[-1,1], node_flow[-1,0]))\n","\n","print(\"Plot Average Stream Flows\")\n","rows = (node_flow.shape[0]//100)+1\n","fig = plt.figure(figsize=(20, 5*rows))\n","fontsize = 10\n","width = 0.4\n","for row in range(rows):\n","    start = 100*row\n","    end = 100*(row+1)\n","    ax = fig.add_subplot(rows,1,row+1)\n","    ax.set_xticks(range(len(node_flow[start:end,0])))\n","    ax.set_xticklabels(node_flow[start:end,0], fontsize=fontsize, rotation=90)\n","    ax.bar(np.array(range(len(node_flow[start:end,0]))), node_flow[start:end,1], width=width , color=\"Green\", label = \"Mean\")\n","    ax.bar(np.array(range(len(node_flow[start:end,0]))), node_flow[start:end,2], width=width , color=\"Blue\", label = \"Median\")\n","    ax.legend(loc=\"upper left\")\n","fig.tight_layout(pad=0)\n","plt.savefig(os.path.join(config.RESULT_DIR, \"Average_Flows_basin.png\"), format = \"png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URqe7FR1T6xG"},"outputs":[],"source":["node = 312\n","feature = 31\n","window = config.window\n","\n","i=0\n","while i < len(data[node,:,-1]):\n","    fig = plt.figure(figsize=(30,3))\n","    fontsize = 20\n","    ax = fig.add_subplot(111)\n","    flow = data[node,:,-1]\n","    flow[flow==config.unknown] = np.nan\n","    sf = ax.plot(flow[i:i+window], color=\"Blue\", label = \"Streamflow\")\n","    lns = sf\n","    \n","    if feature is not None:\n","        ax2 = ax.twinx()\n","        feat = ax2.plot(data[node,:,feature][i:i+window], color=\"orange\", label = \"Feature:{}\".format(feature), alpha = 0.8)\n","        lns = sf+feat\n","    \n","    labs = [l.get_label() for l in lns]\n","    ax.legend(lns, labs, loc=\"upper right\", fontsize=fontsize)\n","    ax.set_xticks(range(0, window, 5))\n","    ax.set_xticklabels(date[i:i+window][::5], rotation=90)\n","    fig.tight_layout(pad=0)\n","    plt.show()\n","    plt.close()\n","    i=i+window"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-Gufr5qT6xH"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"main","language":"python","name":"main"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"name":"DATA_basin_us.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}